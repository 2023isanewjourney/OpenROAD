{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4gZDRBX0EET9Dv+4SMEr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imkiding/OpenROAD/blob/master/SemiGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Installation"
      ],
      "metadata": {
        "id": "-FZeNwna9IDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Streamlit\n",
        "!pip install streamlit\n",
        "\n",
        "# Download Ngrok\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "idWIy0ui-25d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHja9Pp3cUVB"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain streamlit chromadb==0.3.29 unstructured[pdf] tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Definition\n"
      ],
      "metadata": {
        "id": "fn_D7k9K_B-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "# Variables for model selection and setting\n",
        "OPENAI_API_KEY='sk-' # @param {type:\"string\"}\n",
        "\n",
        "Model_Name='gpt-3.5-turbo' # @param [\"gpt-3.5-turbo\", \"2nd option\", \"3rd option\"]\n",
        "\n",
        "Max_Tokens=1000 # @param {type:\"integer\"}\n",
        "\n",
        "Temperature = 0 # @param {type:\"integer\"}\n",
        "\n",
        "class UserInterface:\n",
        "  def get_ui_title(self):\n",
        "    # Chat UI title\n",
        "    st.header(\"Upload Your Files and Ask Questions\")\n",
        "    st.subheader(':open_file_folder: Upload (PDF, DOCX, TXT) Files from the Sidebar :point_left:')\n",
        "\n",
        "  def get_openai_key_in_sidebar(self):\n",
        "    # File uploader in the sidebar on the left\n",
        "    with st.sidebar:\n",
        "      openai_api_key = st.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "      # Set OPENAI_API_KEY as an environment variable\n",
        "      os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    if not openai_api_key:\n",
        "      st.info(\"Please add your OpenAI API key to continue.\")\n",
        "      st.stop()\n",
        "\n",
        "  def update_docs_in_sidebar(self):\n",
        "    with st.sidebar:\n",
        "      uploaded_files = st.file_uploader(\"Please upload your files\", accept_multiple_files=True, type=None)\n",
        "      st.info(\"Please refresh the browser if you decided to upload more files to reset the session\", icon=\"ðŸš¨\")\n",
        "    return uploaded_files\n",
        "\n",
        "  def display_assistant_result(self, result):\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "      message_placeholder = st.empty()\n",
        "      full_response = \"\"\n",
        "      full_response = result[\"answer\"]\n",
        "      message_placeholder.markdown(full_response + \"|\")\n",
        "    message_placeholder.markdown(full_response)\n",
        "    print(full_response)\n",
        "\n",
        "    # Storing this chat history which is crucial for maintaining the context of the conversation.\n",
        "    # It can be used for subsequent questions with the user.\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "\n",
        "class OpenAIModel:\n",
        "\n",
        "  def create_openai_retrieval_chain(self, document_chunks):\n",
        "    if \"processed_data\" not in st.session_state:\n",
        "      embeddings = OpenAIEmbeddings()\n",
        "      vectorstore = Chroma.from_documents(document_chunks, embeddings)\n",
        "      # Store the processed data in session state for reuse\n",
        "      st.session_state.processed_data = {\n",
        "        \"document_chunks\": document_chunks,\n",
        "        \"vectorstore\": vectorstore,\n",
        "      }\n",
        "    else:\n",
        "      # If the processed data is already available, retrieve it from session state\n",
        "      document_chunks = st.session_state.processed_data[\"document_chunks\"]\n",
        "      vectorstore = st.session_state.processed_data[\"vectorstore\"]\n",
        "\n",
        "    # Initialize Langchain's QA Chain with the vectorstore\n",
        "    llm = ChatOpenAI(temperature=Temperature,max_tokens=Max_Tokens, model_name=Model_Name,streaming=True)\n",
        "    retrieval_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
        "    return retrieval_chain\n",
        "\n",
        "  #def create_rag_chain(self):\n",
        "\n",
        "\n",
        "\n",
        "class UserQuestion:\n",
        "\n",
        "  def __init__(self, question):\n",
        "      self.question = question\n",
        "\n",
        "  def create_prompt(self):\n",
        "    # Storing this chat history which is crucial for maintaining the context of the conversation.\n",
        "    # It can be used for subsequent questions with the user.\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": self.question})\n",
        "    with st.chat_message(\"user\"):\n",
        "      st.markdown(self.question)\n",
        "\n",
        "    # Query the assistant using the latest chat history\n",
        "    # Iterates over each message in st.session_state.messages and extracts the \"role\" and \"content\" values from each message.\n",
        "    # For example, if st.session_state.messages contains:\n",
        "    # [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi there\"}],\n",
        "    # then the resulting list would be [(\"user\", \"Hello\"), (\"assistant\", \"Hi there\")].\n",
        "    prompt = {\"question\": self.question, \"chat_history\": [(message[\"role\"], message[\"content\"]) for message in st.session_state.messages]}\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "\n",
        "  def create_document_chuncks(self, uploaded_files):\n",
        "    # Print the number of files to console\n",
        "    print(f\"Number of files uploaded: {len(uploaded_files)}\")\n",
        "\n",
        "    # Load the data and perform preprocessing only if it hasn't been loaded before\n",
        "    # It indicates that the data has not been processed or loaded yet.\n",
        "    # Load the data from uploaded PDF files\n",
        "    documents = []\n",
        "    for uploaded_file in uploaded_files:\n",
        "      # Get the full file path of the uploaded file\n",
        "      file_path = os.path.join(os.getcwd(), uploaded_file.name)\n",
        "\n",
        "      # Save the uploaded file to disk\n",
        "      with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.getvalue())\n",
        "\n",
        "      # Use UnstructuredFileLoader to load the PDF file\n",
        "      loader = UnstructuredFileLoader(file_path)\n",
        "      loaded_documents = loader.load()\n",
        "      print(f\"Number of files loaded: {len(loaded_documents)}\")\n",
        "\n",
        "      # Extend the main documents list with the loaded documents\n",
        "      documents.extend(loaded_documents)\n",
        "\n",
        "    # Chunk the data, create embeddings, and save in vectorstore\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "def main():\n",
        "  user_interface = UserInterface()\n",
        "  user_interface.get_ui_title()\n",
        "  user_interface.get_openai_key_in_sidebar()\n",
        "  # Initialize chat history\n",
        "  if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "  st.write(\"Upload Documents\")\n",
        "  uploaded_files = user_interface.update_docs_in_sidebar()\n",
        "  st.write(\"Upload Documents Done\")\n",
        "\n",
        "  # Check if files are uploaded\n",
        "  if uploaded_files:\n",
        "    # Create an langchain vectorstore\n",
        "    data_professor = DataProcessor()\n",
        "    model = OpenAIModel()\n",
        "    document_chunks = data_professor.create_document_chuncks(uploaded_files)\n",
        "    retrieval_chain = model.create_openai_retrieval_chain(document_chunks)\n",
        "\n",
        "    # Accept and answer user questions\n",
        "    if question:=st.chat_input(\"Your questions?\"):\n",
        "      user_question = UserQuestion(question)\n",
        "      prompt = user_question.create_prompt()\n",
        "\n",
        "      # Display chat messages from history on app rerun\n",
        "      for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "          st.markdown(message[\"content\"])\n",
        "      # Display the prompt for debugging\n",
        "      # st.write(\"Debugging - Prompt:\", prompt)\n",
        "\n",
        "      result = retrieval_chain(prompt)\n",
        "\n",
        "      # Display the anwsers for debugging\n",
        "      # st.write(\"Debugging - Result:\", result)\n",
        "      user_interface.display_assistant_result(result)\n",
        "\n",
        "\n",
        "  else:\n",
        "    st.write(\"Please upload your files.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onjQFZWecpYG",
        "outputId": "0fcda92a-d054-4f1d-9140-64717014faa4",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>  /dev/null&\n",
        "!./ngrok authtoken 2Xzjj6oNCj99MWDAv702AEjeSnl_3zdJ2XVw5rGWbjTb3yuQ9\n",
        "!./ngrok http 8501 &> /dev/null&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbZ-7aKlfVwI",
        "outputId": "d4777e85-8ec4-4f8b-be57-077f11f4c2af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    }
  ]
}